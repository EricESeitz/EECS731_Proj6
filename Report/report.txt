EECS 731 Project 6
D(St)reams of Anomalies
Goal: "Build one or more anomaly detection models to determine the  anomalies  using the other columns as features"

Data from: 
https://www.kaggle.com/boltzmannbrain/nab


3.  Load the  one  of the data set into panda data frames

My first inclination is to look into the "artificialWithAnomaly" dataset as this seems like the simplest place to start with anomoly detection.
Clean, precise data may not be realistic but first starting out I'd like the limit the amount of variables that may futz with my implementation.

The artificialWithAnomaly dataset seems to have a few different .csv files, according to the README.md located within the Data subfolder
"Artifically-generated data with varying types of anomalies."
Pretty straight forward. I'll be choosing one of these .csv files more or less at random, "art_daily_jumpsup.csv" seems as good as any place to start.
Loading it into Pandas I see the dataset isn't very large nor does it have many 'features'. 
It simply as two columns, 'timestamp' and 'value' with 4031 rows. Will this be an issue later? This is something I'll be keeping in mind as I go forward.

4.  Formulate one or two ideas on how feature engineering would help the data set to establish additional value using exploratory data analysis 
This part stumped be a little, wich such limited data and a lack of feratures I'm not sure what else I can do to this dataset.
For now I'm going to continue on, I may load a more "realistic" dataset after this just to try it out but for now building the anomoly detection seems more important.

5.  Build one or more anomaly detection models to determine the  anomalies  using the other columns as features 

Attempt 1:
This Naive Bayes will be under the notebook "EECS731_Proj6_art_NBClassification"

Going from the session slides I'm going to try Naive Bayes first.
I'll be going off of this example to start with:
https://heartbeat.fritz.ai/naive-bayes-classifier-in-python-using-scikit-learn-13c4deb83bcf

I decided to plot the initial data (initialDataPlot.png in Visuals) to see where the spike was located and to get a better idea on the dataset.
Per the example above we're going to predict y using x as an independent variable. In this instance we'll be using timestamp to predict value

First issue I've run into, the 'timestamp' is listed as an object, not a date. I'll have to try and convert this to a proper date format.
Second issue, running the model listed above gives an error:
"UFuncTypeError: ufunc 'add' cannot use operands with types dtype('<M8[ns]') and dtype('<M8[ns]')"

To solve this I'll try converting the dattime to seconds (per: https://www.semicolonworld.com/question/58523/plotting-pandas-timedelta), this should give a float
I did this with initialData['timestamp'].values.astype(float), which converted all the dates to floating point numbers.
This (in theory) shouldn't effect the model, the dates were more for human readability 

This, however, didn't work as the model continued to give me type error:
ValueError: Unknown label type: (array([ 18.00100982,  18.00341479,  18.01353595, ..., 164.33691534,
       164.73603062, 164.93686239]),)

After about 2 hours of troubleshooting I decided to abandon this method, through constant errors and unusable data (See art_y_predPlot.png) I can't figure out how to get any useful information from this naive bayes Gaussian model

I'm going to attempt another model tomorrow.

-------------------------------------------------------------------------------------------------------------------------------------------------------

Attempt 2:
Isolation forest looks interesting to me
Going from here: 
https://blog.paperspace.com/anomaly-detection-isolation-forest/

I'm going to start a new notebook for this one, still using the artifical data as before.
This Isolation forest will be under the notebook "EECS731_Proj6_art_Isolation"

This method seems fairly simple, the first run of it produces some interesting results (See art_isolation_anomaly01.png)
It looks like it's picking out a lot of the outliers but also grabbing "normal" data too judging by the numerous dots under a value of 100.

I'd like to "tighten" up these results or make it more picky on selecting anomalies so we get better outputs.
According to the "accuracy percentage" the first run gives us an accuracy of 374, this tells us it's finding more outlier then actually exist.

With some adjustments to the isolation model (tweaking the "contamination" setting)  we get a much better results (See art_isolation_anomaly02.png)
This resuld isolates the anomolies well with only a few 'acceptable' datapoints also included. I'm much happier with this result.
